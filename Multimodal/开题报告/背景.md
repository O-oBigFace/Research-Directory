# #

>   献给彷徨在未知道路的自己。

[TOC]

## 背景

### 多模态与知识的结合

*   欠缺：目前的VQA数据集、多模态知识图谱数据集、知识图谱综述
*   问题：知识图谱表示到底能不能加入多模态信息？

## 相关工作

### 预训练词向量

本文将使用bert框架来从大规模无监督文本语料中预训练词向量。在过去几年，有许多预训练的方法如Word2Vec, ELMo与GPT，本文会将简单这几种模型的特点并与当下流行的bert进行比较，解释为什么本文选择bert框架。

1.  Word2Vec

    2013年的Word2Vec是一种专门用于预训练词向量的方法，其特点是能够从大规模无监督文本语料中自动获取包含语义信息的词向量。其分为两种模型：CBOW与Skip-gram，前者是利用当前单词的上下文来预测当前词，后者是利用当前词来预测上下文的内容。许多工作使用Word2Vec达到了当时最优的水平。

    从当下的眼光看，Word2Vec有着明显的缺：Word2Vec本质上是静态的向量，即无论使用时文本的上下文是什么，其得到的词向量就不再改变。这种固定的模式使其无法区分多义词的含义。

    虽然从近期的工作中可以看出，Word2Vec正逐渐退出历史舞台，但是其从大规模无监督语料预训练的思想影响了随后的文本预训练方案。

2.  ELMo

    为了解决Word2Vec不能解决多义词的问题，Peters等人提出了ELMo，利用上下文的语义对词的语义来进行区分。

    Word2Vec被提出后，许多任务，包括本文将要提出的方案，都是用了预训练加上特征抽取两个阶段的过程，ELMo也不例外。不过，ELMo作为预训练词向量模型，其输入也是大规模无监督文本语料。在第一个阶段，ELMo先用词嵌入方法来对输入的文本进行编码，这时得到的文本向量无法区分多义词含义；在第二个阶段，ELMo使用双层双向LSTM构建了一个语言模型，该模型的目的根据单词$W_i$的上下文来正确预测$W_i$的值。通过上下文文本语义的帮助，词向量能够正确地确定多义词的含义。

    ELMo预训练的步骤不仅产生了更加优质的词向量，同时还得到了一个训练好的双层双向LSTM模型。使用时，一个新的句子输入到ELMo后，可以得到三个向量：原始词向量、两个双向LSTM向量，这三个向量在NLP任务中有着不同的效用。ELMo刷新了当时阅读理解、句子语义关系判断、分类任务、实体识别等NLP任务的最高水平。

    从模型中可以发现，ELMo依旧采用了串联双向RNN以获得上下文的语义。相较于随后提出的Transformer，RNN的运行效率和特征抽取能力要明显落后。

3.  GPT

    GPT的思想和ELMo类似，都采用了两个阶段的特征抽取过程。和ELMo不同的是，GPT在第二个阶段中使用特征抽取能力更强的Transformer替换了RNN。并且，GPT在第二阶段中，只利用了上文而没有使用双向的信息来对当前单词进行预测。虽然只利用了单向的文本信息，但是Transformer强大的特征抽取能力使GPT的表现超过了ELMo模型。

    在预训练好词向量之后，GPT允许模型根据任务进行参数的微调。这种方式将在预训练时学习到的语言知识运用到当前的任务当中。实验结果表明这种预训练加微调的方式在各类NLP任务中都达到了当时的最好效果。

    虽然GPT的预训练加微调的方式有着较好的表现，但是它使用的是单向的语言模型，只能接受上文中的语义信息。这会对某些任务如阅读理解产生不良的影响。

4.  BERT

    基于上述的预训练语言模型，谷歌提出了新的语言表示模型BERT。不同于之前的语言表示模型，BERT旨在通过联合调节所有层中的左右上下文来预训练深度双向表示。因此，只需要一个额外的输出层，就可以对预训练的BERT表示进行微调。

    在训练过程中，BERT在两个任务上进行训练："遮蔽语言模型"和"预测下一个句子"。"遮蔽语言模型"让模型预测被遮蔽的标记使模型拥有接收双向语义信息的能力，"预测下一个句子"任务使模型能够把握整体语义。

    实验结果表明，经过微调后，BERT在11种自然语言处理任务上刷新了现有记录。

### 字符嵌入

#### 编辑距离

编辑距离是对两个字符串差异程度的量化，量化方式是计算当前字符串需要进行多少次处理才能将一个字符串转变为另一个字符串。其中，**Levenshtein**距离是使用比较广泛的一种测量方法，在该方法中，允许字符串的编辑操作有插入字符(情况1)、删除字符(情况2)、某些字符被替换(情况3)。

使用编辑距离可以实现不完全匹配的字符串的查找，但是，从计算方式上来看，编辑距离有着以下一些缺点：

1.  计算中需要同时依赖于两个字符串，复杂度很高。即我们根据一个指称在知识图谱中查找对应实体的计算复杂度较高；
2.  编辑距离对某些情况的适配性并不是很好。例如：在社交网络中，人们经常会将一个单词的某个字母重复多次来表达情绪，这时，计算编辑距离的方法很难将该指称与实体名称相对应。

#### 字符嵌入 (Character Embedding)

字符嵌入最初在"Text Understanding from Scratch"中被提出，最初是用来解决单词形态的多样性导致的out-of-vocabulary的问题，在随后的论文"Exploring the Limits of Language Modeling"，作者通过实验发现通过字符嵌入，能够解决两个字符串名称不对应的情况。实验结果表明，使用字符嵌入不仅仅能够解决OOV的问题，对低频词的识别也有着较好的效果。

字符嵌入的一般方法是先通过预先定义的字符表，将某个单词的所有字符都转化为向量。然后利用某种特征提取器对该向量进行特征抽取。这样，单词就转化成了向量的形式。从形式上来看，字符嵌入与词嵌入有着很高的相似性，但是，它们之间有着一些明显的区别：

1.  字符嵌入得到的词向量不包含该单词的语义信息，而词嵌入生成的向量包含了该单词的语义信息，而构建候选实体列表时我们尽量不要参考实体的语义；
2.  单词由有限的字符构成，几乎所有的单词都可以使用字符嵌入的到词向量；而词嵌入法只关注词汇表中的单词，而生僻词和不在词汇表中的单词的效果较差。
3.  字符嵌入词汇表的向量数目较少，维度较小，而词嵌入生成的向量数目很大，而且维度一般很高。

所以，在MNED中，词嵌入方法并不适合用来构建候选实体库，而字符嵌入的方法具有很高的灵活性，与该任务相适配。

与上述编辑距离相比，字符嵌入有着以下一些优势：

1.  可以预先训练好所有的字符嵌入，训练时不需要两个字符串同时参与。计算相似度时，只需要将当前单词的字符嵌入和知识图谱中实体的已有字符嵌入比较，大大减少了计算轮次；
2.  字符嵌入可以捕捉单词的形态学特征，可以解决一些特殊情况（如某个字符重复多次）；而编辑距离无法达到该效果。

#####  

## 

## 多模态

人类能够结合多个模态的信号来认识、理解自然事物，如驾驶汽车时结合视觉和听觉信号判断路况，倾听时通过发音、口型判断对方说话内容等。

人类期望计算机拥有智能。传感器的发明让计算机拥有了感官，自然语言处理与计算机视觉等单模态学习技术的研究赋予了计算机感知能力。当下，学者们努力地将现有的感知智能发展为更高阶的认知智能，即让计算机拥有思考的能力。多模态学习正脱胎于这个想法，试图还原人脑的认知过程：它允许计算机像人类一样同时接收、处理多模态信号，使计算机能够真正地理解现象。

## 知识图谱

>   现在就是总结，总结就是现在。

###概念



1.  背景

    Google在2012年提出的概念，最初被用来提高旗下搜索引擎的语义搜索能力，增强产品的搜索质量并提高用户的搜索体验。目前的知识图谱是由语义网发展而来，它以实体-关系网络的形式来抽象地表示物理世界。从认知的角度上看，知识图谱是对人类拥有知识这一现象的模拟。人们希望机器能够借助知识图谱，解决一些认知层面的任务。

2.  定义
    目前的知识图谱可被看作是结构化的语义知识库，从逻辑上看，它由本体层和实例层两部分构成，本体层是结构化的知识库概念模板，实例层则包含了一系列的事实，通常将概念和实例统称为实体；从构成方式上看，知识图谱是对物理世界的一个符号表达，其可以用<实体，关系(属性)，实体(字面量)>三元组表示，实体之间通过关系相互连通；从研究价值上看，知识图谱用最小的代价将实体连接起来，成为可被利用的知识；从应用价值上来看，它能够为现有的应用提供结构化的知识。

4.  困难

    *   构建知识图谱

    *   已经构成的知识图谱不完备

        *   缺少事实，需要补全
        *   缺少多模态数据

        ### 发展

        

###构建-和NLP领域的交织

*   构建-和NLP领域的交织

    *   知识抽取
    *   实体识别 ★
        *   关系抽取
    *   知识融合

        *   实体链接 ★

    #### 知识抽取

    >   一种⾃动化地从半结构化和无结构数据中抽取实体、关系以及实体属性等结构化信息的技术。——刘峤等 2016

    ##### 命名实体识别 Named Entity Recognization

    ###### 定义

    命名实体识别(实体识别、实体分块、实体提取)是信息抽取的一个子任务，旨在将文本中的命名实体**定位**并**分类**为预先定义的类别。

    

    ###### 

    

    

    

    ###知识图谱的意义和应用

*   知识图谱的意义及应用

    *   搜索引擎
    *   自然语言理解：使用知识图谱来理解文本中的语义信息
    *   问答系统：可通过它进行语义推理
    *   推荐系统
    *   ...

## 知识图谱和多模态的结合

1.  实体识别
2.  表示学习
3.  VQA

## 引用

**Review on Knowledge Graph Techniques**

