# #综述

[TOC]

## 现状

### 多模态辅助NLP任务

目前，已经有许多工作正在研究多模态与知识图谱的交叉领域。知识图谱作为重要的外部资料，为NLP和CV任务提供了重要的外部知识。下面将展开介绍近期知识图谱和多模态信息结合的任务。

#### 联合表示学习

Xie等人[Xie et al., 2017](https://arxiv.org/pdf/1609.07028.pdf)提出多模态知识图谱联合表示学习模型IKRL。作者认为，实体对应的图像能够为现有的知识图谱表示扩充丰富的视觉信息。于是，作者先为现有知识图谱中的实体添加相应的图片，并在TransE的基础上添加了图片提供的视觉信息。实验效果证明，视觉信息的加入，能够使学习到的表示在TranE的基础上提高1~2个百分点。

如图1所示，该工作将图片中的包含关系或是视觉上的相似关系加入到知识图谱表示当中。思路相仿的论文还有[Sergieh et al., 2018](https://pdfs.semanticscholar.org/be91/946bedbf65d543a7eb9dd1e033e7aaf78c3c.pdf?_ga=2.239204043.953046097.1557891086-1745154373.1553132250)，[Pezeshkpour et al., 2018 ](https://aclweb.org/anthology/D18-1359)。
![图1 图片描述了盔甲和头盔的关系（图片来源：Xie et al., 2017）](../%E7%9F%A5%E4%B9%8E%E6%8F%90%E9%97%AE%E4%BB%A5%E5%8F%8A%E5%BD%A2%E6%88%90%E6%80%BB%E7%BB%93/assets/image-20190516110513384.png)

#### 3.1.2 命名实体链接

CMU在ACL18发表的[Moon et al., 2018a](https://vitordecarvalho.github.io/papers/acl2018_NED.pdf)，在推特带有图片的推文上作实体链接。如图2所示，该工作的思路是利用图像中包含的视觉上下文信息来帮助实体消岐。在推文中，由于文本上下文的缺失，无法用传统的方法来对当前实体消岐，而替代地利用图像中的实体作为上下文。
![图2 传统的实体链接通过文本上下文来判断实体的类型，社交网络没有上下文的情况下，可以将图片看作为上下文辅助实体链接 （图片来源：Moon et al., 2018a）](../%E7%9F%A5%E4%B9%8E%E6%8F%90%E9%97%AE%E4%BB%A5%E5%8F%8A%E5%BD%A2%E6%88%90%E6%80%BB%E7%BB%93/assets/image-20190516110602455.png)

#### 3.1.3 命名实体识别

命名实体识别是从原始文本中构建知识图谱的前置任务，所以将其放在这里。

CMU在NAACL18上的[Moon et al., 2018b](https://www.aclweb.org/anthology/N18-1078)和复旦大学在AAAI18上发表的[Zhang et al., 2018](https://pdfs.semanticscholar.org/5ca1/c595708d22d92b3f913be391575560bdab2c.pdf?_ga=2.258610882.953046097.1557891086-1745154373.1553132250)都实现了推特中的带有图片推文的命名实体识别。利用图片附加的信息，我们可以判断文本中一些实体的类型。图3放上Zhang et al., 2018中的一个生动又典型的例子：如果没有图像的帮助，文本中的"Rocky"就会被识别为人名。
![图3 "Rocky"是一条狗狗的名字 (图片来源：Zhang et al., 2018）](../%E7%9F%A5%E4%B9%8E%E6%8F%90%E9%97%AE%E4%BB%A5%E5%8F%8A%E5%BD%A2%E6%88%90%E6%80%BB%E7%BB%93/assets/image-20190516144053222.png)



#### 3.1.4 多模态知识图谱

现有的知识图谱如FREEBASE、YAGO等等主要由文本构成，其他类型的信息一般会用url链接的形式存储。

### 3.2 文本语义信息辅助CV任务

实际上，目前多模态领域更多的是用文本的语义信息来辅助CV领域的任务，如图像识别([Srivastava et al., 2012](http://papers.nips.cc/paper/4683-multimodal-learning-with-deep-boltzmann-machines.pdf)，[Frome et al., 2013)](http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf)、图像关系侦测(relation detection)([Lu et al., 2016](https://link.springer.com/content/pdf/10.1007%2F978-3-319-46448-0.pdf))、零样本迁移学习([Socher et al., 2013](http://papers.nips.cc/paper/5027-zero-shot-learning-through-cross-modal-transfer.pdf))、VQA([Lu et al., 2016](https://arxiv.org/pdf/1606.00061.pdf))等等。在这些任务中，这些词嵌入包含的语义信息对CV的识别有提升作用。



## 4. 难点

从前面提到的一些工作来看，多模态信息的加入能够使传统的知识图谱相关应用的效果得到改进，并且能够实现一些以往方法不能完成的功能。但是，目前知识图谱和多模态交叉领域的工作数量较少，而且没有形成一个较为通用的解决方案。本文从近期的工作中，总结多模态与知识图谱交叉难点如下：

1.  以目前的研究水平来看，其他模态如视觉信息，能够提供的信息非常有限

    知识图谱相关的应用多为NLP任务。尽管上面介绍了一些用视觉辅助NLP任务的例子，然而目前CV领域的研究还停留在偏"感知"的分类、识别上，提供的信息也只有图片实体类型信息、属性信息、位置信息等等，这些对NLP中的偏"认知"任务的帮助是很小的。

    -   在[Xie et al., 2017](https://arxiv.org/pdf/1609.07028.pdf)的任务中，定义的实体之间的"关系"是很有限的，只有"has part", "part of", "synset domain topic of"等等简单的关系；
    -   [Moon et al., 2018a](https://vitordecarvalho.github.io/papers/acl2018_NED.pdf)和[Moon et al., 2018b](https://www.aclweb.org/anthology/N18-1078)中，直接先将图片中的实体识别出来，再利用实体的类型信息辅助具体任务。

2.  视觉信息的识别仅限于概念信息，而非实例信息

    我们知道，知识图谱"重实例，轻本体"，在文本中，我们将一个实体识别出来之后，利用一些字符串匹配方法，很容易地就可以将该实体链接到知识图谱的一条实例上去。而从图片中，我们只能识别出实体的概念。

    >   ![图4 一条多模态信息 (图片来源:(../%E7%9F%A5%E4%B9%8E%E6%8F%90%E9%97%AE%E4%BB%A5%E5%8F%8A%E5%BD%A2%E6%88%90%E6%80%BB%E7%BB%93/assets/image-20190516115205288.png)](assets/image-20190516115205288.png)
    >
    >   "<u>特朗普</u>当选了美国总统。"

    在图4的一条多模态信息中，我们很容易地将识别出的文本实体"特朗普"链接到知识图谱上。而利用图像识别，我们只能识别出概念"人"。

3.  目前的研究点还比较狭隘，很难继续深入

    这一点和第1点很相似，受CV现阶段的研究水平所限，基于视觉信息的NLP任务无法有很大的突破。而且，现阶段相关任务的**数据集**还很少，无法支持这方面的研究。

虽然多模态和知识图谱交叉的任务有着许多挑战，但是从现有的工作来看，多模态数据在特定场景下对某些任务效果的提升是显著的。并且，目前已经有工作构建了多模态知识图谱，并证明了多模态数据对知识图谱的实例匹配和知识补全任务有着促进作用。